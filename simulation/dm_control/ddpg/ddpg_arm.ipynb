{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dm_control import viewer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from simulation.dm_control.ddpg.ddpg import DDPGagent, OUNoise\n",
    "import simulation.dm_control.simulation_control.environments as environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "PATH_MODEL = 'ddpg_actor.pt'\n",
    "NUM_EPISODES = 500\n",
    "BATCH_SIZE = 128\n",
    "DURATION = 200\n",
    "ACTOR_LEARNING_RATE=1e-4\n",
    "CRITIC_LEARNING_RATE=1e-3\n",
    "GAMMA=0.99\n",
    "TAU=1e-2\n",
    "\n",
    "env = environments.load(domain_name='passive_hand', task_name='lift_sparse')\n",
    "action_spec = env.action_spec()\n",
    "dim_action = action_spec.shape[0]\n",
    "dim_obs = 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "agent = DDPGagent(\n",
    "    dim_obs,\n",
    "    dim_action,\n",
    "    actor_learning_rate=ACTOR_LEARNING_RATE,\n",
    "    critic_learning_rate=CRITIC_LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    tau=TAU\n",
    ")\n",
    "noise = OUNoise(dim_action, action_spec.minimum, action_spec.maximum)\n",
    "\n",
    "def denorm(a): #  use on model output before passing to env\n",
    "    act_k = (action_spec.maximum - action_spec.minimum) / 2.\n",
    "    act_b = (action_spec.maximum + action_spec.minimum) / 2.\n",
    "    return a * act_k + act_b\n",
    "\n",
    "def norm(a): # use on env output before passing to model\n",
    "    act_k_inv = 2. / (action_spec.maximum - action_spec.minimum)\n",
    "    act_b = (action_spec.maximum + action_spec.minimum) / 2.\n",
    "    return act_k_inv * (a - act_b)\n",
    "\n",
    "def parse(obs):\n",
    "    \"\"\"\n",
    "    Take only gripper position and object position???\n",
    "    \"\"\"\n",
    "    grip_pos = obs['grip_pos']\n",
    "    object_pos = obs['object_pos']\n",
    "    return np.append(grip_pos, object_pos)\n",
    "\n",
    "def calc_reward(obs):\n",
    "    \"\"\"\n",
    "    Calculate the reward based on stuffs\n",
    "    observation=OrderedDict(\n",
    "    [('grip_pos', array([1.38313716, 0.74702476, 0.58570326])),\n",
    "    ('grip_velp', array([-1.02185151,  0.06579234, -0.62871064])),\n",
    "    ('grip_velr', array([-0.01182298,  0.16122969, -0.00438519])),\n",
    "    ('grip_rot', array([-0.00114049, -0.01091738,  0.00111363])),\n",
    "    ('object_pos', array([1.45921682, 0.74832965, 0.41110006])),\n",
    "    ('object_rel_pos', array([ 0.07607965,  0.00130489, -0.1746032 ])),\n",
    "    ('object_velp', array([-0.10321769, -0.1453771 , -0.1968166 ])),\n",
    "    ('object_velr', array([ 1.90336989, -0.97895901, -8.12644349])),\n",
    "    ('object_rel_velp', array([ 0.91863382, -0.21116944,  0.43189404])),\n",
    "    ('simulation_time', 0.12000000000000009)]))\n",
    "    \"\"\"\n",
    "    grip_pos = obs['grip_pos']\n",
    "    object_pos = obs['object_pos']\n",
    "    obj_height = object_pos[2] - 0.41110006\n",
    "    rel_dist = np.sum((object_pos - grip_pos) ** 2)**(1/2)\n",
    "    # print(f'h: {obj_height}, d: {rel_dist}, r: {obj_height - rel_dist}')\n",
    "    return obj_height - rel_dist\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]c:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 22%|██▏       | 108/500 [05:17<21:58,  3.36s/it]WARNING:absl:Nan, Inf or huge value in QACC at DOF 35. The simulation is unstable. Time = 3.6460.\n",
      " 22%|██▏       | 111/500 [05:26<20:49,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: -97.27, average_reward: nan\n",
      "episode: 1, reward: -117.83, average_reward: -97.27305677930904\n",
      "episode: 2, reward: -92.38, average_reward: -107.55364877345659\n",
      "episode: 3, reward: -153.12, average_reward: -102.4953980628493\n",
      "episode: 4, reward: -104.81, average_reward: -115.15163667034633\n",
      "episode: 5, reward: -131.17, average_reward: -113.08317737813145\n",
      "episode: 6, reward: -228.97, average_reward: -116.09690395391101\n",
      "episode: 7, reward: -231.22, average_reward: -132.22161001836182\n",
      "episode: 8, reward: -228.21, average_reward: -144.5966594262432\n",
      "episode: 9, reward: -221.41, average_reward: -153.8872847844104\n",
      "episode: 10, reward: -211.41, average_reward: -160.63986992346207\n",
      "episode: 11, reward: -114.14, average_reward: -172.0537181940212\n",
      "episode: 12, reward: -70.0, average_reward: -171.6842842785997\n",
      "episode: 13, reward: -150.38, average_reward: -169.44594957236828\n",
      "episode: 14, reward: -98.1, average_reward: -169.17230978810807\n",
      "episode: 15, reward: -89.0, average_reward: -168.50109461112532\n",
      "episode: 16, reward: -66.15, average_reward: -164.2847379384587\n",
      "episode: 17, reward: -180.15, average_reward: -148.00297881642905\n",
      "episode: 18, reward: -96.65, average_reward: -142.89617555801203\n",
      "episode: 19, reward: -123.7, average_reward: -129.7402536876478\n",
      "episode: 20, reward: -118.81, average_reward: -119.96880850811003\n",
      "episode: 21, reward: -94.77, average_reward: -110.70846859413916\n",
      "episode: 22, reward: -67.88, average_reward: -108.77157591732218\n",
      "episode: 23, reward: -62.68, average_reward: -108.56017597632051\n",
      "episode: 24, reward: -58.3, average_reward: -99.78985447886558\n",
      "episode: 25, reward: -62.1, average_reward: -95.81055989616453\n",
      "episode: 26, reward: -65.16, average_reward: -93.12065083660308\n",
      "episode: 27, reward: -61.76, average_reward: -93.02187906242222\n",
      "episode: 28, reward: -57.77, average_reward: -81.1822770341724\n",
      "episode: 29, reward: -58.59, average_reward: -77.294095166542\n",
      "episode: 30, reward: -47.62, average_reward: -70.78326758089891\n",
      "episode: 31, reward: -158.68, average_reward: -63.66485358637874\n",
      "episode: 32, reward: -55.63, average_reward: -70.05588727310109\n",
      "episode: 33, reward: -54.0, average_reward: -68.83079540461793\n",
      "episode: 34, reward: -67.48, average_reward: -67.96301520874954\n",
      "episode: 35, reward: -54.34, average_reward: -68.88077086824407\n",
      "episode: 36, reward: -51.33, average_reward: -68.10430159800839\n",
      "episode: 37, reward: -58.25, average_reward: -66.72041034144976\n",
      "episode: 38, reward: -196.68, average_reward: -66.36985396435122\n",
      "episode: 39, reward: -85.75, average_reward: -80.26099159372585\n",
      "episode: 40, reward: -78.15, average_reward: -82.9774177761264\n",
      "episode: 41, reward: -50.4, average_reward: -86.02995371762884\n",
      "episode: 42, reward: -49.25, average_reward: -75.20165875248436\n",
      "episode: 43, reward: -173.83, average_reward: -74.56313530784298\n",
      "episode: 44, reward: -40.5, average_reward: -86.54614393267907\n",
      "episode: 45, reward: -52.97, average_reward: -83.84793462635778\n",
      "episode: 46, reward: -89.81, average_reward: -83.71147981910272\n",
      "episode: 47, reward: -85.54, average_reward: -87.55942170645442\n",
      "episode: 48, reward: -67.04, average_reward: -90.28826687147809\n",
      "episode: 49, reward: -61.64, average_reward: -77.32424775488539\n",
      "episode: 50, reward: -68.87, average_reward: -74.91302598835652\n",
      "episode: 51, reward: -49.38, average_reward: -73.98524076201038\n",
      "episode: 52, reward: -27.27, average_reward: -73.88299749547961\n",
      "episode: 53, reward: -108.69, average_reward: -71.685951103454\n",
      "episode: 54, reward: -22.53, average_reward: -65.17146222297171\n",
      "episode: 55, reward: -30.19, average_reward: -63.3741024921041\n",
      "episode: 56, reward: -28.43, average_reward: -61.09614627450867\n",
      "episode: 57, reward: -31.15, average_reward: -54.959049141509034\n",
      "episode: 58, reward: -36.69, average_reward: -49.52018789831956\n",
      "episode: 59, reward: -38.46, average_reward: -46.4853295717311\n",
      "episode: 60, reward: -33.07, average_reward: -44.16695974032835\n",
      "episode: 61, reward: -41.47, average_reward: -40.58661008468175\n",
      "episode: 62, reward: -101.04, average_reward: -39.79617876640965\n",
      "episode: 63, reward: -41.84, average_reward: -47.17222993826863\n",
      "episode: 64, reward: -108.74, average_reward: -40.487208785821714\n",
      "episode: 65, reward: -132.99, average_reward: -49.1083440595668\n",
      "episode: 66, reward: -98.15, average_reward: -59.388370769197856\n",
      "episode: 67, reward: -86.24, average_reward: -66.36035936257134\n",
      "episode: 68, reward: -41.24, average_reward: -71.86866533729429\n",
      "episode: 69, reward: -43.8, average_reward: -72.32296125674668\n",
      "episode: 70, reward: -45.34, average_reward: -72.85705605552165\n",
      "episode: 71, reward: -40.12, average_reward: -74.08387576729345\n",
      "episode: 72, reward: -134.96, average_reward: -73.94910691938155\n",
      "episode: 73, reward: -35.3, average_reward: -77.34143683208154\n",
      "episode: 74, reward: -41.96, average_reward: -76.6881108244459\n",
      "episode: 75, reward: -64.34, average_reward: -70.01034228253786\n",
      "episode: 76, reward: -52.84, average_reward: -63.14520590506923\n",
      "episode: 77, reward: -153.33, average_reward: -58.613424419544096\n",
      "episode: 78, reward: -21.66, average_reward: -65.32299470037158\n",
      "episode: 79, reward: -121.01, average_reward: -63.36578184642237\n",
      "episode: 80, reward: -150.94, average_reward: -71.0868985313275\n",
      "episode: 81, reward: -83.14, average_reward: -81.64761111373384\n",
      "episode: 82, reward: -81.91, average_reward: -85.94930394993088\n",
      "episode: 83, reward: -24.66, average_reward: -80.6440462767576\n",
      "episode: 84, reward: -37.67, average_reward: -79.57940824958034\n",
      "episode: 85, reward: -39.94, average_reward: -79.15038602854456\n",
      "episode: 86, reward: -40.23, average_reward: -76.70997618663617\n",
      "episode: 87, reward: -49.16, average_reward: -75.44909992663383\n",
      "episode: 88, reward: -33.22, average_reward: -65.03160702426834\n",
      "episode: 89, reward: -49.4, average_reward: -66.18682045541344\n",
      "episode: 90, reward: -84.75, average_reward: -59.02559689186355\n",
      "episode: 91, reward: -74.83, average_reward: -52.40581709196638\n",
      "episode: 92, reward: -44.87, average_reward: -51.57501156647503\n",
      "episode: 93, reward: -54.46, average_reward: -47.87175990441288\n",
      "episode: 94, reward: -104.91, average_reward: -50.85229915566013\n",
      "episode: 95, reward: -83.83, average_reward: -57.57653095441445\n",
      "episode: 96, reward: -56.77, average_reward: -61.96561788961064\n",
      "episode: 97, reward: -50.63, average_reward: -63.61965907383329\n",
      "episode: 98, reward: -37.38, average_reward: -63.76751635259681\n",
      "episode: 99, reward: -98.67, average_reward: -64.18369124538756\n",
      "episode: 100, reward: -119.9, average_reward: -69.11058920279643\n",
      "episode: 101, reward: -53.41, average_reward: -72.62571207610306\n",
      "episode: 102, reward: -133.76, average_reward: -70.4830304446155\n",
      "episode: 103, reward: -51.99, average_reward: -79.37186789931013\n",
      "episode: 104, reward: -112.93, average_reward: -79.12476358198283\n",
      "episode: 105, reward: -199.79, average_reward: -79.92654399002615\n",
      "episode: 106, reward: -48.26, average_reward: -91.52267300157959\n",
      "episode: 107, reward: -82.77, average_reward: -90.6716577240677\n",
      "Physics Error: [ 1.          0.81775524  1.         -1.          0.37589985]\n",
      "episode: 108, reward: -237.42, average_reward: -93.88517877488434\n",
      "episode: 109, reward: -91.05, average_reward: -113.88945855581399\n",
      "episode: 110, reward: -110.73, average_reward: -113.12746678185013\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "avg_rewards = []\n",
    "\n",
    "for episode in tqdm(range(NUM_EPISODES)):\n",
    "    time_step = env.reset()\n",
    "    state = parse(time_step.observation)\n",
    "    noise.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(DURATION):\n",
    "        action = agent.get_action(state)\n",
    "        action = noise.get_action(action, step)\n",
    "        try:\n",
    "            time_step_2 = env.step(denorm(action))\n",
    "        except:\n",
    "            print(f'Physics Error: {action}')\n",
    "            continue\n",
    "        state_2 = parse(time_step_2.observation)\n",
    "        reward = calc_reward(time_step_2.observation)\n",
    "        agent.memory.push(state, action, reward, state_2, -1)\n",
    "        state = state_2\n",
    "        if len(agent.memory) > BATCH_SIZE:\n",
    "            agent.update(BATCH_SIZE)\n",
    "        episode_reward += reward\n",
    "    print(f\"episode: {episode}, \"\n",
    "      f\"reward: {np.round(episode_reward, decimals=2)}, \"\n",
    "      f\"average_reward: {np.mean(rewards[-10:])}\")\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "\n",
    "agent.save(PATH_MODEL)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.load(PATH_MODEL)\n",
    "# Define a uniform random policy.\n",
    "def random_policy(time_step):\n",
    "    state = parse(time_step.observation)\n",
    "    x = denorm(agent.get_action(state))\n",
    "    print(x)\n",
    "    return x\n",
    "\n",
    "# Launch the viewer application.\n",
    "viewer.launch(env, policy=random_policy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}